{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d44e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from IPython.utils import io\n",
    "\n",
    "from functions import butter_bandpass, butter_bandpass_filter\n",
    "from augmentation import pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc59a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation results are first obtaned from BSG portal\n",
    "# This is currently not openly available\n",
    "\n",
    "# exclude non-bird test sites from data \n",
    "excluded_sites=[10001, 10002, 10003, 10005, 10006, 10007] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efbd1e",
   "metadata": {},
   "source": [
    "# BSG templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c88fca9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 2289\n",
      "Saving annotated BSG templates...\n",
      "File 759: Species names not matching: Oressochen jubatus <-> Neochen jubata\n",
      "File 760: Species names not matching: Oressochen jubatus <-> Neochen jubata\n",
      "File 766: Species names not matching: Oressochen jubatus <-> Neochen jubata\n",
      "File 767: Species names not matching: Oressochen jubatus <-> Neochen jubata\n",
      "File 3345: Species names not matching: Eupodotis savilei <-> Lophotis savilei\n",
      "File 3346: Species names not matching: Eupodotis savilei <-> Lophotis savilei\n",
      "File 3347: Species names not matching: Eupodotis savilei <-> Lophotis savilei\n",
      "File 3348: Species names not matching: Eupodotis savilei <-> Lophotis savilei\n",
      "File 3349: Species names not matching: Eupodotis savilei <-> Lophotis savilei\n",
      "File 3350: Species names not matching: Eupodotis savilei <-> Lophotis savilei\n",
      "File 3351: Species names not matching: Eupodotis savilei <-> Lophotis savilei\n",
      "File 3352: Species names not matching: Eupodotis savilei <-> Lophotis savilei\n",
      "File 3353: Species names not matching: Eupodotis savilei <-> Lophotis savilei\n",
      "File 3359: Species names not matching: Eupodotis gindiana <-> Lophotis gindiana\n",
      "File 3360: Species names not matching: Eupodotis gindiana <-> Lophotis gindiana\n",
      "File 3363: Species names not matching: Eupodotis gindiana <-> Lophotis gindiana\n",
      "File 3364: Species names not matching: Eupodotis gindiana <-> Lophotis gindiana\n",
      "File 4947: Species names not matching: Mentocrex kioloides <-> Mentrocrex kioloides\n",
      "File 4948: Species names not matching: Mentocrex kioloides <-> Mentrocrex kioloides\n",
      "File 4949: Species names not matching: Mentocrex kioloides <-> Mentrocrex kioloides\n",
      "File 4950: Species names not matching: Mentocrex kioloides <-> Mentrocrex kioloides\n",
      "File 4951: Species names not matching: Mentocrex kioloides <-> Mentrocrex kioloides\n",
      "File 4952: Species names not matching: Mentocrex kioloides <-> Mentrocrex kioloides\n",
      "File 4953: Species names not matching: Mentocrex kioloides <-> Mentrocrex kioloides\n",
      "File 4954: Species names not matching: Mentocrex kioloides <-> Mentrocrex kioloides\n",
      "File 4955: Species names not matching: Mentocrex kioloides <-> Mentrocrex kioloides\n",
      "File 4956: Species names not matching: Mentocrex kioloides <-> Mentrocrex kioloides\n",
      "File 5065: Species names not matching: Mustelirallus albicollis <-> Porzana albicollis\n",
      "File 5066: Species names not matching: Mustelirallus albicollis <-> Porzana albicollis\n",
      "File 5067: Species names not matching: Mustelirallus albicollis <-> Porzana albicollis\n",
      "File 5073: Species names not matching: Mustelirallus erythrops <-> Neocrex erythrops\n",
      "File 5074: Species names not matching: Mustelirallus erythrops <-> Neocrex erythrops\n",
      "File 5075: Species names not matching: Mustelirallus erythrops <-> Neocrex erythrops\n",
      "File 5079: Species names not matching: Mustelirallus erythrops <-> Neocrex erythrops\n",
      "File 5080: Species names not matching: Mustelirallus erythrops <-> Neocrex erythrops\n",
      "File 5350: Species names not matching: Zapornia flavirostra <-> Amaurornis flavirostra\n",
      "File 5351: Species names not matching: Zapornia flavirostra <-> Amaurornis flavirostra\n",
      "File 5352: Species names not matching: Zapornia flavirostra <-> Amaurornis flavirostra\n",
      "File 5353: Species names not matching: Zapornia flavirostra <-> Amaurornis flavirostra\n",
      "File 9272: Species names not matching: Ciccaba huhula <-> Strix huhula\n",
      "File 9273: Species names not matching: Ciccaba huhula <-> Strix huhula\n",
      "File 9274: Species names not matching: Ciccaba huhula <-> Strix huhula\n",
      "File 9275: Species names not matching: Ciccaba huhula <-> Strix huhula\n",
      "File 10657: Species names not matching: Dryobates fumigatus <-> Leuconotopicus fumigatus\n",
      "File 10658: Species names not matching: Dryobates fumigatus <-> Leuconotopicus fumigatus\n",
      "File 10659: Species names not matching: Dryobates fumigatus <-> Leuconotopicus fumigatus\n",
      "File 10660: Species names not matching: Dryobates fumigatus <-> Leuconotopicus fumigatus\n",
      "File 10661: Species names not matching: Dryobates fumigatus <-> Leuconotopicus fumigatus\n",
      "File 10662: Species names not matching: Dryobates fumigatus <-> Leuconotopicus fumigatus\n",
      "File 10663: Species names not matching: Dryobates fumigatus <-> Leuconotopicus fumigatus\n",
      "File 10664: Species names not matching: Dryobates fumigatus <-> Leuconotopicus fumigatus\n",
      "File 10709: Species names not matching: Dryobates frontalis <-> Veniliornis frontalis\n",
      "File 10710: Species names not matching: Dryobates frontalis <-> Veniliornis frontalis\n",
      "File 10711: Species names not matching: Dryobates frontalis <-> Veniliornis frontalis\n",
      "File 10712: Species names not matching: Dryobates frontalis <-> Veniliornis frontalis\n",
      "File 10713: Species names not matching: Dryobates frontalis <-> Veniliornis frontalis\n",
      "File 10714: Species names not matching: Dryobates frontalis <-> Veniliornis frontalis\n",
      "File 12914: Species names not matching: Sylviorthorhynchus yanacensis <-> Leptasthenura yanacensis\n",
      "File 12915: Species names not matching: Sylviorthorhynchus yanacensis <-> Leptasthenura yanacensis\n",
      "File 16262: Species names not matching: Rhodophoneus cruentus <-> Telophorus cruentus\n",
      "File 16263: Species names not matching: Rhodophoneus cruentus <-> Telophorus cruentus\n",
      "File 16264: Species names not matching: Rhodophoneus cruentus <-> Telophorus cruentus\n",
      "File 16265: Species names not matching: Rhodophoneus cruentus <-> Telophorus cruentus\n",
      "File 17979: Species names not matching: Pygochelidon cyanoleuca <-> Notiochelidon cyanoleuca\n",
      "File 17980: Species names not matching: Pygochelidon cyanoleuca <-> Notiochelidon cyanoleuca\n",
      "File 17998: Species names not matching: Orochelidon andecola <-> Haplochelidon andecola\n",
      "File 22601: Species names not matching: Piranga flava <-> Piranga hepatica\n",
      "File 22602: Species names not matching: Piranga flava <-> Piranga hepatica\n",
      "File 22604: Species names not matching: Piranga flava <-> Piranga hepatica\n",
      "File 22703: Species names not matching: Cyanoloxia brissonii <-> Cyanocompsa brissonii\n",
      "File 22908: Species names not matching: Pipraeidea bonariensis <-> Thraupis bonariensis\n",
      "File 22909: Species names not matching: Pipraeidea bonariensis <-> Thraupis bonariensis\n",
      "File 22910: Species names not matching: Pipraeidea bonariensis <-> Thraupis bonariensis\n",
      "File 22911: Species names not matching: Pipraeidea bonariensis <-> Thraupis bonariensis\n",
      "Complete!                             \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"BSG_data/bsg_results.tsv\", sep = \"\\t\")\n",
    "data['url'] = [f.split(\".laji.fi/\")[1][0:7] for f in data['url']]\n",
    "metadata = pd.read_csv(\"BSG_data/BSG_template_files_metadata.csv\") \n",
    "\n",
    "path_in = \"---/\"  # path to original audio files from xeno-canto/Macaulay library\n",
    "path_to_birdsound_library = \"---/\" # path where created templates are saved\n",
    "path_to_template_metadata = \"---/bsg_templates.csv\" # path where template metadata is saved\n",
    "\n",
    "sp_ind = 0\n",
    "prev_sp = \"\"\n",
    "print(\"Saving annotated BSG templates...\")\n",
    "n_row = len(data)\n",
    "template_metadata = pd.DataFrame()\n",
    "\n",
    "for i in range(n_row):\n",
    "    splitted = data['file_name'].iloc[i].split('_')\n",
    "    sp = data['scientific_name'].iloc[i]\n",
    "    source = data['url'].iloc[i]\n",
    "    sp_code = data['species_code'].iloc[i]\n",
    "    start_min = 0\n",
    "    if  source == 'xeno-ca': # source: xeno_canto\n",
    "        source = 'xenocanto'\n",
    "        fid = splitted[0].split('.')[0] \n",
    "    elif source == 'cornell': # source: Macaulay library\n",
    "        source = 'macaulay'\n",
    "        fid = splitted[0]\n",
    "        end = splitted[1].split('.')[0]\n",
    "        start_min = int(end)\n",
    "    else:\n",
    "        print(\"Invalid source!\")\n",
    "    sp_sci = metadata['Species_sci'].loc[(metadata['File'] == int(fid)) & (metadata['Source']==source)].iloc[0]\n",
    "    if(sp_sci != sp):\n",
    "        print(f\"File {i}: Species names not matching: {sp} <-> {sp_sci}\")\n",
    "    path = metadata['filename'].loc[(metadata['File'] == int(fid)) & (metadata['Source']==source)].iloc[0]\n",
    "    path_full = path_in + source + '/' + path \n",
    "    x1 = data['x1'].iloc[i]\n",
    "    x2 = data['x2'].iloc[i]\n",
    "    y1 = data['y1'].iloc[i]\n",
    "    y2 = data['y2'].iloc[i]\n",
    "    if(x2-x1 > 3): # define the start and end of the clip\n",
    "        start = x1\n",
    "        stop = x2\n",
    "    else:\n",
    "        start = np.max([((x2+x1)/2)-1.5,0])\n",
    "        stop = x1+3\n",
    "    with io.capture_output() as captured:\n",
    "        sig, sr = librosa.load(path_full, sr = 48000, offset = start_min*600+start, duration = stop-start) # original version\n",
    "    sig2 = pad(sig, x1- start, x2-start, target_len = len(sig), sr=48000) # time- and frequency cropped version\n",
    "    sig2 = butter_bandpass_filter(sig2, [y1,y2], 48000, 12) \n",
    "    # SAVE\n",
    "    if(sp_code == prev_sp):\n",
    "        sp_ind = sp_ind +1\n",
    "    else:\n",
    "        prev_sp = sp_code\n",
    "        sp_ind = 1\n",
    "    filename_out = sp_code + '_' + str(sp_ind)\n",
    "    sig = librosa.resample(sig, orig_sr=48000, target_sr=24000) # resample to 24 kHz\n",
    "    sig2 = librosa.resample(sig2, orig_sr=48000, target_sr=24000)\n",
    "    sf.write(path_to_birdsound_library + filename_out + '_orig.wav', sig, 24000)\n",
    "    sf.write(path_to_birdsound_library + filename_out + '_cleaned.wav', sig2, 24000)\n",
    "    # METADATA\n",
    "    template_metadata = pd.concat([template_metadata, pd.DataFrame({\"Filename\": [filename_out + '_orig.wav', filename_out + '_cleaned.wav'], \n",
    "                                                                    \"Species_code\": [sp_code, sp_code], \"Species\": [sp, sp], \n",
    "                                                                    \"original\":[1, 0], \"file_id\":[fid, fid], \"source\":[source, source]})])\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}/{n_row} ({round(100*(i/n_row), 2)} %)...\", end='\\r')\n",
    "        template_metadata.to_csv(path_to_template_metadata, index = False)\n",
    "\n",
    "template_metadata.to_csv(path_to_template_metadata, index = False)\n",
    "print(\"Complete!                             \")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafca375",
   "metadata": {},
   "source": [
    "# BSG soundscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4732efc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from all users collected.            \n",
      "Combining annotations...\n",
      "All annotations combined.                                    \n"
     ]
    }
   ],
   "source": [
    "# Collect data from all users to one file \n",
    "species_annotations_path = 'BSG_data/bsg_identification_results/species_annotations/' \n",
    "annotations_path = 'BSG_data/bsg_identification_results/annotations/' \n",
    "users = os.listdir(species_annotations_path)\n",
    "\n",
    "additional_taxa = {'Bombina variegata': ['fr_ybtoad', 'Yellow-bellied Toad'],\n",
    "                   'Barbastella barbastellus':['bat_1', 'bat_1'], 'Eptesicus nilssonii':['bat_2', 'bat_2'],'Eptesicus serotinus':['bat_3', 'bat_3'],\n",
    "                   'Hypsugo savii':['bat_4', 'bat_4'], 'Miniopterus schreibersii':['bat_5', 'bat_5'],'Myotis alcathoe':['bat_6', 'bat_6'], \n",
    "                   'Myotis blythii':['bat_7', 'bat_7'], 'Myotis capaccinii':['bat_8', 'bat_8'],'Myotis crypticus':['bat_9', 'bat_9'], \n",
    "                   'Myotis daubentonii':['bat_10', 'bat_10'], 'Nyctalus lasiopterus':['bat_11', 'bat_11'], 'Nyctalus leisleri':['bat_12', 'bat_12'], \n",
    "                   'Nyctalus noctula':['bat_13', 'bat_13'], 'Pipistrellus kuhlii':['bat_14', 'bat_14'], 'Pipistrellus nathusii':['bat_15', 'bat_15'],\n",
    "                   'Pipistrellus pipistrellus':['bat_16', 'bat_16'], 'Pipistrellus pygmaeus':['bat_17', 'bat_17'], 'Plecotus auritus':['bat_18', 'bat_18'], \n",
    "                   'Plecotus austriacus':['bat_19', 'bat_19'],'Rhinolophus euryale':['bat_20', 'bat_20'], 'Rhinolophus ferrumequinum':['bat_21', 'bat_21'],\n",
    "                   'Rhinolophus hipposideros':['bat_22', 'bat_22'], 'Tadarida teniotis':['bat_23', 'bat_23'], 'Vespertilio murinus':['bat_24', 'bat_24'],\n",
    "                   'Rhinolophus\\xa0mehelyi':['bat_25', 'bat_25'], 'Mix Myo50':['bat_26', 'bat_26'], 'Mix Myo30':['bat_27', 'bat_27'], 'Plecotus sp':['bat_28', 'bat_28'],\n",
    "                   'Mix EptNycVes':['bat_29', 'bat_29'], 'Mix TadNyc':['bat_30', 'bat_30'],\n",
    "                   'Aethiomerus madagassus':['bug_1', 'bug_1'], 'Ambylakis sp':['bug_2', 'bug_2'], 'Listroscelidinae ngen nsp':['bug_3', 'bug_3'],\n",
    "                   'Neozvenella nsp':['bug_4', 'bug_4'], 'Odontolakis sp':['bug_5', 'bug_5'], 'Odontolakis virescens':['bug_6', 'bug_6'],\n",
    "                   'Oecanthus brevicauda':['bug_7', 'bug_7'],'Parasimodera nsp':['bug_8', 'bug_8'], 'Paragryllodes sp':['bug_9', 'bug_9']}\n",
    "\n",
    "data_10s = pd.DataFrame()\n",
    "\n",
    "for i, u in enumerate(users):\n",
    "    user_data1 = pd.read_csv(species_annotations_path + u, sep = '\\t')\n",
    "    user_data2 = pd.read_csv(annotations_path + u, sep = '\\t')\n",
    "    d ={\"species_annotation_id\":'', \"recording_id\":user_data2['recording_id'], \"annotation_id\":user_data2['annotation_id'], \n",
    "       \"species_code\":\"other\", \"scientific_name\":\"Other\", \"common_name\":\"Other\", \"occurrence\":user_data2['contains_unknown_birds'].astype('int')}\n",
    "    user_data1 = pd.concat([user_data1, pd.DataFrame(d)], ignore_index=True)\n",
    "    user_data1['id_by'] = u[:-4]\n",
    "    user_data1['occurrence'] = user_data1['occurrence'].replace(2, 0.5)\n",
    "    # manually fix non-bird taxa\n",
    "    if any(user_data1.isna().any()):\n",
    "        for j in range(len(user_data1)):\n",
    "            if pd.isna(user_data1['species_code'].iloc[j]):\n",
    "                user_data1.loc[j, 'species_code'] = additional_taxa[user_data1['scientific_name'].iloc[j]][0]\n",
    "                user_data1.loc[j, 'common_name'] = additional_taxa[user_data1['scientific_name'].iloc[j]][1]\n",
    "    data_10s = pd.concat([data_10s, user_data1], ignore_index = True)\n",
    "    print(f\"Processed user {u} ({i+1}/{len(users)})     \", end = \"\\r\")\n",
    "print(\"Data from all users collected.       \")\n",
    "\n",
    "# Combine the data from different users\n",
    "\n",
    "print(\"Combining annotations...\")\n",
    "final_data = pd.DataFrame()\n",
    "rec_ids = np.unique(data_10s['recording_id'])\n",
    "n_rec = len(rec_ids)\n",
    "count = 0\n",
    "for rec_id in rec_ids : # loop through recordings\n",
    "    rec_data = data_10s.loc[data_10s['recording_id'] == rec_id]\n",
    "    for sp in np.unique(rec_data['species_code']): # loop through species\n",
    "        if (sp == \"other\"): # save whether there are other species or not, and the number of users who agree on this\n",
    "            other_sps_occ = np.min(rec_data['occurrence'].loc[rec_data['species_code'] == \"other\"]) \n",
    "            d = {\"species_code\":[sp], \"recording_id\":rec_id, \"occurrence\":other_sps_occ, \n",
    "                   \"n_users\":len(rec_data.loc[((rec_data['species_code'] == \"other\") & (rec_data['occurrence'] == other_sps_occ))])}\n",
    "            final_data = pd.concat([final_data, pd.DataFrame(d)])\n",
    "        else: \n",
    "            sp_occ = []\n",
    "            for u in np.unique(rec_data['id_by']): # loop through users\n",
    "                user_sp_id = rec_data[((rec_data['species_code'] == sp) & (rec_data['id_by'] == u))]['occurrence']\n",
    "                if len(user_sp_id)>0:\n",
    "                    sp_occ.append(user_sp_id.iloc[0]) # Collect all annotations from different users for the same species\n",
    "                    if(len(user_sp_id)>1):\n",
    "                        print(\"MAYDAY!!\")\n",
    "                elif (rec_data['occurrence'].loc[((rec_data['species_code'] == \"other\") & (rec_data['id_by']==u))].iloc[0]==0):\n",
    "                    sp_occ.append(0) # Save negative identification if user states that the recording does not contain other species \n",
    "            d = {\"species_code\":[sp], \"recording_id\":rec_id, \"occurrence\":np.mean(sp_occ), \n",
    "                   \"n_users\":len(sp_occ)}\n",
    "            final_data = pd.concat([final_data, pd.DataFrame(d)])\n",
    "    count = count+1\n",
    "    if count % 100 == 0:\n",
    "        print(f\"Processed {count}/{len(rec_ids)} ({round(count/len(rec_ids)*100, 2)} %) recordings\", end = \"\\r\")\n",
    "print(\"All annotations combined.                                    \")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83249870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10s annotations saved.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species_code</th>\n",
       "      <th>recording_id</th>\n",
       "      <th>occurrence</th>\n",
       "      <th>n_users</th>\n",
       "      <th>site_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>start_buff</th>\n",
       "      <th>sample</th>\n",
       "      <th>end_buff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>houspa</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1162</td>\n",
       "      <td>A-1162-22_0182d4ec-50af-4028-a980-868fc6b17f8a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1162</td>\n",
       "      <td>A-1162-22_0182d4ec-50af-4028-a980-868fc6b17f8a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eursta</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1162</td>\n",
       "      <td>A-1162-22_0182d4ec-50af-4028-a980-868fc6b17f8a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1162</td>\n",
       "      <td>A-1162-22_0182d4ec-50af-4028-a980-868fc6b17f8a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eursta</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1162</td>\n",
       "      <td>A-1162-22_0182d4ec-50af-4028-a980-868fc6b17f8a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90356</th>\n",
       "      <td>eurtre1</td>\n",
       "      <td>276718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10011</td>\n",
       "      <td>Phylloscopus_sibilatrix_Troglodytes_troglodyte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90357</th>\n",
       "      <td>other</td>\n",
       "      <td>276718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10011</td>\n",
       "      <td>Phylloscopus_sibilatrix_Troglodytes_troglodyte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90358</th>\n",
       "      <td>redcro</td>\n",
       "      <td>276718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10011</td>\n",
       "      <td>Phylloscopus_sibilatrix_Troglodytes_troglodyte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90359</th>\n",
       "      <td>spofly1</td>\n",
       "      <td>276718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10011</td>\n",
       "      <td>Phylloscopus_sibilatrix_Troglodytes_troglodyte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90360</th>\n",
       "      <td>winwre4</td>\n",
       "      <td>276718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10011</td>\n",
       "      <td>Phylloscopus_sibilatrix_Troglodytes_troglodyte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79898 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      species_code  recording_id  occurrence  n_users  site_id  \\\n",
       "0           houspa            18         1.0        1     1162   \n",
       "1            other            18         0.0        1     1162   \n",
       "2           eursta            40         1.0        1     1162   \n",
       "3            other            40         0.0        1     1162   \n",
       "4           eursta            42         1.0        1     1162   \n",
       "...            ...           ...         ...      ...      ...   \n",
       "90356      eurtre1        276718         1.0        1    10011   \n",
       "90357        other        276718         0.0        1    10011   \n",
       "90358       redcro        276718         1.0        1    10011   \n",
       "90359      spofly1        276718         1.0        1    10011   \n",
       "90360      winwre4        276718         1.0        1    10011   \n",
       "\n",
       "                                               file_name  start_buff  sample  \\\n",
       "0      A-1162-22_0182d4ec-50af-4028-a980-868fc6b17f8a...         5.0    10.0   \n",
       "1      A-1162-22_0182d4ec-50af-4028-a980-868fc6b17f8a...         5.0    10.0   \n",
       "2      A-1162-22_0182d4ec-50af-4028-a980-868fc6b17f8a...         5.0    10.0   \n",
       "3      A-1162-22_0182d4ec-50af-4028-a980-868fc6b17f8a...         5.0    10.0   \n",
       "4      A-1162-22_0182d4ec-50af-4028-a980-868fc6b17f8a...         5.0    10.0   \n",
       "...                                                  ...         ...     ...   \n",
       "90356  Phylloscopus_sibilatrix_Troglodytes_troglodyte...         0.0    60.0   \n",
       "90357  Phylloscopus_sibilatrix_Troglodytes_troglodyte...         0.0    60.0   \n",
       "90358  Phylloscopus_sibilatrix_Troglodytes_troglodyte...         0.0    60.0   \n",
       "90359  Phylloscopus_sibilatrix_Troglodytes_troglodyte...         0.0    60.0   \n",
       "90360  Phylloscopus_sibilatrix_Troglodytes_troglodyte...         0.0    60.0   \n",
       "\n",
       "       end_buff  \n",
       "0           5.0  \n",
       "1           5.0  \n",
       "2           5.0  \n",
       "3           5.0  \n",
       "4           5.0  \n",
       "...         ...  \n",
       "90356       0.0  \n",
       "90357       0.0  \n",
       "90358       0.0  \n",
       "90359       0.0  \n",
       "90360       0.0  \n",
       "\n",
       "[79898 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add metadata for files and save the data \n",
    "\n",
    "recordings_path = 'BSG_data/bsg_identification_results/recordings.tsv' \n",
    "mp3_buffers_path = 'BSG_data/mp3_20s.txt'\n",
    "\n",
    "recordings = pd.read_csv(recordings_path, sep = '\\t')\n",
    "recording_splits = pd.read_csv(mp3_buffers_path, sep = ' ', header = None, names = ['file_name', 'start_buff', 'sample', 'end_buff'])\n",
    "recordings = recordings[['recording_id', 'site_id', 'file_name']]\n",
    "final_data = final_data.merge(recordings, on='recording_id', how='left')\n",
    "final_data = final_data.merge(recording_splits, on='file_name', how='left')\n",
    "\n",
    "final_data = final_data.loc[~final_data['site_id'].isin(excluded_sites)] # remove data from excluded sites\n",
    "\n",
    "missing_metadata = final_data.loc[final_data['start_buff'].isna()]\n",
    "if(len(missing_metadata) > 0):\n",
    "    print(f\"Check that BSG_data/mp3_20s.txt is up to date. Metadata missing for {len(missing_metadata)} files:\")\n",
    "    print(np.unique(missing_metadata['file_name']))\n",
    "\n",
    "final_data.to_csv(\"BSG_results/10s_annotations.csv\", index = False)\n",
    "print(\"10s annotations saved.\")\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3bd9d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing box data...\n",
      "  Fixing names for non-bird taxa...\n",
      "  Joining boxes...\n",
      "Box data preprocessing complete.\n",
      "Saving clips...\n",
      "    Processing empty clips...\n",
      "    Processing non-bird sounds...                             \n",
      "    Processing bird vocalizations...             \n",
      "Clips complete.                                                               \n"
     ]
    }
   ],
   "source": [
    "# find and save clips\n",
    "\n",
    "path_species = 'BSG_data/bsg_identification_results/species_annotations/'\n",
    "path_boxes = 'BSG_data/bsg_identification_results/species_annotation_boxes/'\n",
    "path_info = 'BSG_data/bsg_identification_results/annotations/'\n",
    "path_recordings = 'BSG_data/bsg_identification_results/recordings.tsv'\n",
    "path_mp3_buffers = 'BSG_data/mp3_20s.txt'\n",
    "\n",
    "lifeplan_path = '---/' # path to lifeplan soundscapes\n",
    "non_lifeplan_path = '---/' # path to non-lifeplan soundscapes\n",
    "path_to_birdsound_library = '---/' # path where created clips are saved\n",
    "\n",
    "raw_box_data_out = 'BSG_results/raw_box_data.csv'\n",
    "processed_box_data_out = 'BSG_results/processed_box_data.csv'\n",
    "bsg_metadata_out = '---/' # path where clip metadata is saved\n",
    "\n",
    "recordings = pd.read_csv(path_recordings, sep = \"\\t\")\n",
    "recordings = recordings[['recording_id','site_id','file_name']]\n",
    "recordings = recordings.loc[~recordings['site_id'].isin(excluded_sites)] # remove data from excluded sites\n",
    "\n",
    "######################################################################\n",
    "# Preprocess boxes and join small boxes that are close to each other #\n",
    "######################################################################\n",
    "\n",
    "print(\"Preprocessing box data...\")\n",
    "# Combine data from all users\n",
    "file_list = os.listdir(path = path_boxes) # list users who have made identifications\n",
    "data = pd.DataFrame()\n",
    "for i in range(len(file_list)):\n",
    "    temp_data1 = pd.read_csv(path_boxes + file_list[i], sep = '\\t') # read user-specific files\n",
    "    temp_data2 = pd.read_csv(path_species + file_list[i], sep = '\\t')\n",
    "    temp_data3 = pd.read_csv(path_info + file_list[i], sep = '\\t')\n",
    "    temp_data3 = temp_data3[['recording_id', 'has_boxes_for_all_bird_sounds']]\n",
    "    temp_data = pd.merge(temp_data1, temp_data2, on=\"species_annotation_id\", how='left') # merge files\n",
    "    temp_data = pd.merge(temp_data, temp_data3, on=\"recording_id\", how='left')\n",
    "    temp_data['occurrence'] = temp_data['occurrence'].replace(2, 0.5) # unsure annotations correspond to 0.5\n",
    "    data = pd.concat([data, temp_data]) # join with data from other users\n",
    "data=data.reset_index(drop=True)\n",
    "data.to_csv(raw_box_data_out, index=False)\n",
    "\n",
    "# Combine boxes from same species that are next to each other\n",
    "# If several users have drawn different boxes for the same species, the boxes will be joined here\n",
    "data = data.sort_values(by=['recording_id','species_code', 'area_x1'], ascending = [True, True, True])\n",
    "data = pd.concat([data, pd.DataFrame({\"species_annotation_id\":[0], \"area_x1\":[0], \"area_x2\":[0], \"area_y1\":[0], \"area_y2\":[0], \"overlaps_with_other_species\":False, \"recording_id\":[0], \"annotation_id\":[0], \n",
    "                                      \"species_code\":\"last_row\", \"scientific_name\":\"last_row\", \"common_name\":\"last_row\", \"occurrence\":[0], \"has_boxes_for_all_bird_sounds\":False})])\n",
    "data=data.reset_index(drop=True)\n",
    "# manually fix non-bird taxa\n",
    "print(\"  Fixing names for non-bird taxa...\")\n",
    "if any(data.isna().any()):\n",
    "    for j in range(len(data)):\n",
    "        if pd.isna(data['species_code'].iloc[j]):\n",
    "            data.loc[j, 'species_code'] = additional_taxa[data['scientific_name'].iloc[j]][0]\n",
    "            data.loc[j, 'common_name'] = additional_taxa[data['scientific_name'].iloc[j]][1]\n",
    "check_data_len = len(data)\n",
    "data['sound_type'] = data['sound_type'].fillna(0) # replace unnecessary sound_type na:s with 0\n",
    "data = data.dropna()\n",
    "check_data_len2 = len(data)\n",
    "if check_data_len != check_data_len2:\n",
    "    print(f\"Omitted {check_data_len - check_data_len2} rows due to NAs.\")\n",
    "data = data.loc[data['recording_id'].isin(recordings['recording_id'])] # remove data from excluded sites\n",
    "data=data.reset_index(drop=True)\n",
    "\n",
    "print(\"  Joining boxes...\")\n",
    "annotations_10s = pd.read_csv(\"BSG_results/10s_annotations.csv\")\n",
    "i = 0\n",
    "prev_rec = 0\n",
    "row_idxs = []\n",
    "while(True):\n",
    "    if(i == len(data)-1):\n",
    "        break\n",
    "    j = i\n",
    "    while((data['recording_id'].iloc[j+1] == data['recording_id'].iloc[j]) & (data['species_code'].iloc[j+1] == data['species_code'].iloc[j]) & (data['area_x1'].iloc[j+1]-data['area_x2'].iloc[j]<1)):\n",
    "        j = j+1\n",
    "    if data['recording_id'].iloc[j] != prev_rec: # select all boxes from same recording for overlapping comparison\n",
    "        prev_rec = data['recording_id'].iloc[j]\n",
    "        rec_data = data.loc[data['recording_id'] == prev_rec]\n",
    "    sub_data = data.iloc[i:j+1] # select all boxes to join\n",
    "    #save box only if majority of annotators agrees that species is present in recording\n",
    "    if annotations_10s['occurrence'].loc[((annotations_10s['species_code']==data['species_code'].iloc[i]) & (annotations_10s['recording_id']==data['recording_id'].iloc[i]))].iloc[0] >= 0.5:\n",
    "        if j > i: #if there are several boxes to join, calculate new metadata for the box\n",
    "            data.loc[i,'area_x1'] = np.min(sub_data['area_x1'])\n",
    "            data.loc[i, 'area_x2'] = np.max(sub_data['area_x2'])\n",
    "            data.loc[i, 'area_y1'] = np.min(sub_data['area_y1'])\n",
    "            data.loc[i, 'area_y2'] = np.max(sub_data['area_y2'])\n",
    "            data.loc[i, 'overlaps_with_other_species'] = sub_data['overlaps_with_other_species'].any()\n",
    "            data.loc[i, 'occurrence'] = np.max(sub_data['occurrence'])\n",
    "            rec_data_other_sps = rec_data.loc[rec_data['species_code']!=data['species_code'].iloc[i]] # Check if the resulting box overlaps with boxes of other species\n",
    "            for sub_i in range(len(rec_data_other_sps)):\n",
    "                if (((data.loc[i, 'area_x2']> rec_data_other_sps['area_x1'].iloc[sub_i]) & (data.loc[i, 'area_y2']> rec_data_other_sps['area_y1'].iloc[sub_i])) & ((data.loc[i, 'area_x1']< rec_data_other_sps['area_x2'].iloc[sub_i]) & (data.loc[i, 'area_y1']< rec_data_other_sps['area_y2'].iloc[sub_i]))):\n",
    "                    data.loc[i, 'overlaps_with_other_species'] = True\n",
    "        row_idxs.append(i)\n",
    "    i = j+1\n",
    "joint_boxes_data=data.iloc[row_idxs]\n",
    "joint_boxes_data.reset_index(drop=True, inplace=True)\n",
    "joint_boxes_data=pd.merge(joint_boxes_data, recordings, on=\"recording_id\", how='left')\n",
    "joint_boxes_data.to_csv(processed_box_data_out, index=False)\n",
    "print(\"Box data preprocessing complete.\")\n",
    "\n",
    "###############################\n",
    "# Save clips and their labels #\n",
    "###############################\n",
    "bsg_metadata = pd.DataFrame()\n",
    "bsg_labels = pd.DataFrame()\n",
    "\n",
    "print(\"Saving clips...\")\n",
    "running_idx = 1\n",
    "\n",
    "# Empty and non-bird clips\n",
    "file_list = os.listdir(path_info)\n",
    "rec_info_data = pd.DataFrame()\n",
    "for i in range(len(file_list)):\n",
    "    temp_data = pd.read_csv(path_info + file_list[i], sep='\\t')\n",
    "    rec_info_data = pd.concat([rec_info_data, temp_data])\n",
    "rec_info_data=pd.merge(rec_info_data, recordings, on=\"recording_id\", how='right')\n",
    "\n",
    "print(\"    Processing empty clips...\")\n",
    "# Choose only recordings, where all agree that no birds occur: sort by does_not_contain_birds and drop duplicates\n",
    "empty_data = rec_info_data.sort_values('does_not_contain_birds', ascending=True)\n",
    "empty_data.reset_index(drop=True, inplace=True)\n",
    "empty_data.drop_duplicates(subset=['recording_id'], keep='first', inplace=True)\n",
    "empty_data = empty_data.loc[((empty_data['does_not_contain_birds']==True)&(empty_data['contains_unknown_birds']==False))] # filter by both attributes just in case\n",
    "empty_data = empty_data[~empty_data['recording_id'].isin(joint_boxes_data['recording_id'])] # filter out those cases where somebody has still drawn boxes for birds\n",
    "empty_data.reset_index(drop=True, inplace=True)\n",
    "mp3_buffers_path = 'BSG_data/mp3_20s.txt'\n",
    "recording_splits = pd.read_csv(mp3_buffers_path, sep = ' ', header = None, names = ['file_name', 'start_buff', 'sample', 'end_buff'])\n",
    "empty_data = empty_data.merge(recording_splits, on='file_name', how='left')\n",
    "\n",
    "for i in range(len(empty_data)): # Clip to 4s frames and save\n",
    "    buffer = empty_data['start_buff'].iloc[i]\n",
    "    sample_len = empty_data['sample'].iloc[i]\n",
    "    x1 = buffer\n",
    "    while(x1+4 < buffer+sample_len):\n",
    "        if os.path.isfile(lifeplan_path + empty_data['file_name'].iloc[i]):\n",
    "            with io.capture_output() as captured:\n",
    "                sig, sr = librosa.load(lifeplan_path + empty_data['file_name'].iloc[i], sr = 24000, offset = x1, duration = 4)\n",
    "        else:\n",
    "            with io.capture_output() as captured:\n",
    "                sig, sr = librosa.load(non_lifeplan_path + empty_data['file_name'].iloc[i], sr = 24000, offset = x1, duration = 4)\n",
    "        file_out = str(empty_data['recording_id'].iloc[i]) + '_' + str(round(x1,1)) + '_' + str(running_idx) + '.wav'\n",
    "        sf.write(path_to_birdsound_library + file_out, sig, 24000)\n",
    "        running_idx = running_idx+1  \n",
    "        bsg_metadata = pd.concat([bsg_metadata, pd.DataFrame({\"file_name\":file_out, \"site_id\":[empty_data['site_id'].iloc[i]], \n",
    "                                                                   \"cleaned\":False, \"no_other_species\":True, \"noise\":True})])\n",
    "        bsg_labels = pd.concat([bsg_labels, pd.DataFrame({\"file_name\":file_out, \"site_id\":[empty_data['site_id'].iloc[i]], \n",
    "                                                        \"species\":\"nobird\", \"occurrence\":[1]})])\n",
    "        x1=x1+3\n",
    "        if i % 100 == 0:\n",
    "            print(f\"    Processed {i}/{len(empty_data)} ({round(i/len(empty_data)*100, 2)} %) clips   \", end = \"\\r\")\n",
    "      \n",
    "                                                          \n",
    "print(\"    Processing non-bird sounds...                             \")\n",
    "non_bird_data = rec_info_data.dropna(subset=['non_bird_area_x1'])\n",
    "non_bird_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for i in range(len(non_bird_data)): # loop through non-bird boxes\n",
    "    file_name = non_bird_data['file_name'].iloc[i]\n",
    "    full_rec_length = recording_splits[['start_buff', 'sample', 'end_buff']].loc[recording_splits['file_name']==file_name].sum(1).iloc[0]\n",
    "    x1 = non_bird_data['non_bird_area_x1'].iloc[i]\n",
    "    x2 = non_bird_data['non_bird_area_x2'].iloc[i]\n",
    "    if x2-x1 > 4:\n",
    "        start = x1\n",
    "        stop = x2\n",
    "    else:\n",
    "        mean_x = np.mean([x1, x2])\n",
    "        start = mean_x -2\n",
    "        stop = mean_x + 2\n",
    "    if start < 0:\n",
    "        start = 0\n",
    "        stop = np.max([stop, 3])\n",
    "    if stop > full_rec_length:\n",
    "        stop = full_rec_length\n",
    "        start = np.min([start, full_rec_length-3])\n",
    "    if os.path.isfile(lifeplan_path + file_name):\n",
    "        with io.capture_output() as captured:\n",
    "            sig, sr = librosa.load(lifeplan_path + file_name, sr = 48000, offset = start, duration = stop-start)\n",
    "    else:\n",
    "        with io.capture_output() as captured:\n",
    "            sig, sr = librosa.load(non_lifeplan_path + file_name, sr = 48000, offset = start, duration = stop-start)\n",
    "    sig = pad(sig, x1-start, x2-start, snr=1, target_len=len(sig), sr=48000) # time- and frequency cropped version\n",
    "    sig = butter_bandpass_filter(sig, [non_bird_data['non_bird_area_y1'].iloc[i],non_bird_data['non_bird_area_y2'].iloc[i]], 48000, 12) \n",
    "    sig = librosa.resample(sig, orig_sr=48000, target_sr=24000)\n",
    "    file_out = str(non_bird_data['recording_id'].iloc[i]) + '_' + str(round(start,1)) + '_' + str(running_idx) + '.wav'\n",
    "    sf.write(path_to_birdsound_library + file_out, sig, 24000)\n",
    "    running_idx = running_idx+1  \n",
    "    bsg_metadata = pd.concat([bsg_metadata, pd.DataFrame({\"file_name\":file_out, \"site_id\":[non_bird_data['site_id'].iloc[i]], \n",
    "                                                                   \"cleaned\":True, \"no_other_species\":True, \"noise\":True})])\n",
    "    bsg_labels = pd.concat([bsg_labels, pd.DataFrame({\"file_name\":file_out, \"site_id\":[non_bird_data['site_id'].iloc[i]], \n",
    "                                                        \"species\":\"nobird\", \"occurrence\":[1]})])\n",
    "    if i % 100 == 0:\n",
    "        print(f\"    Processed {i}/{len(non_bird_data)} ({round(i/len(non_bird_data)*100, 2)} %) clips   \", end = \"\\r\")\n",
    "                                                      \n",
    "\n",
    "print(\"    Processing bird vocalizations...             \")\n",
    "prev_rec=0\n",
    "for i in range(len(joint_boxes_data)):\n",
    "    if joint_boxes_data['recording_id'].iloc[i] != prev_rec: # select all boxes from same recording for overlapping comparison\n",
    "        prev_rec = joint_boxes_data['recording_id'].iloc[i]\n",
    "        rec_data = joint_boxes_data.loc[joint_boxes_data['recording_id'] == prev_rec]\n",
    "    # Define start and stop for the clip\n",
    "    file_name = joint_boxes_data['file_name'].iloc[i]\n",
    "    full_rec_length = recording_splits[['start_buff', 'sample', 'end_buff']].loc[recording_splits['file_name']==file_name].sum(1).iloc[0]\n",
    "    x1 = joint_boxes_data['area_x1'].iloc[i]\n",
    "    x2 = joint_boxes_data['area_x2'].iloc[i]\n",
    "    if x2-x1 > 3:\n",
    "        start = x1\n",
    "        stop = x2\n",
    "    else:\n",
    "        mean_x = np.mean([x1, x2])\n",
    "        start = mean_x -2\n",
    "        stop = mean_x + 2\n",
    "    if start < 0:\n",
    "        start = 0\n",
    "        stop = np.max([stop, 3])\n",
    "    if stop > full_rec_length:\n",
    "        stop = full_rec_length\n",
    "        start = np.min([start, full_rec_length-3])\n",
    "    if os.path.isfile(lifeplan_path + file_name):\n",
    "        with io.capture_output() as captured:\n",
    "            sig, sr = librosa.load(lifeplan_path + file_name, sr = 48000, offset = start, duration = stop-start)\n",
    "    else:\n",
    "        with io.capture_output() as captured:\n",
    "            sig, sr = librosa.load(non_lifeplan_path + file_name, sr = 48000, offset = start, duration = stop-start)\n",
    "    # collect information of all species occurring in the clip\n",
    "    sps = rec_data.loc[((rec_data['area_x1'] < stop-1) & (rec_data['area_x2'] > start+1))]\n",
    "    sps = sps.sort_values('occurrence', ascending=False)\n",
    "    sps.reset_index(drop=True, inplace=True)\n",
    "    sps.drop_duplicates(subset=['species_code'], keep='first', inplace=True)\n",
    "    # load signal\n",
    "    sig1 = librosa.resample(sig, orig_sr=48000, target_sr=24000)\n",
    "    file_out = str(joint_boxes_data['recording_id'].iloc[i]) + '_' + str(round(start,1)) + '_' + str(running_idx) + '.wav'\n",
    "    sf.write(path_to_birdsound_library + file_out, sig1, 24000)\n",
    "    running_idx = running_idx+1 \n",
    "    bsg_metadata = pd.concat([bsg_metadata, pd.DataFrame({\"file_name\":file_out, \"site_id\":[joint_boxes_data['site_id'].iloc[i]], \n",
    "                                                        \"cleaned\":False, \"no_other_species\":joint_boxes_data['has_boxes_for_all_bird_sounds'].iloc[i], \n",
    "                                                          \"noise\":False})])\n",
    "    bsg_labels = pd.concat([bsg_labels, pd.DataFrame({\"file_name\":file_out, \"site_id\":sps['site_id'], \n",
    "                                                        \"species\":sps['species_code'], \"occurrence\":sps['occurrence']})])    \n",
    "    # if box is clean, save cleaned clip\n",
    "    if(joint_boxes_data['overlaps_with_other_species'].iloc[i] == False):\n",
    "        sig2 = pad(sig, x1-start, x2-start, snr=10, target_len=len(sig), sr=48000) # time- and frequency cropped version\n",
    "        sig2 = butter_bandpass_filter(sig2, [joint_boxes_data['area_y1'].iloc[i],joint_boxes_data['area_y2'].iloc[i]], 48000, 12) \n",
    "        sig2 = librosa.resample(sig2, orig_sr=48000, target_sr=24000)\n",
    "        file_out = str(joint_boxes_data['recording_id'].iloc[i]) + '_' + str(round(start,1)) + '_' + str(running_idx) + '_cleaned.wav'\n",
    "        sf.write(path_to_birdsound_library + file_out, sig2, 24000)\n",
    "        running_idx = running_idx+1  \n",
    "        bsg_metadata = pd.concat([bsg_metadata, pd.DataFrame({\"file_name\":file_out, \"site_id\":[joint_boxes_data['site_id'].iloc[i]], \n",
    "                                                        \"cleaned\":True, \"no_other_species\":True, \"noise\":False})])\n",
    "        bsg_labels = pd.concat([bsg_labels, pd.DataFrame({\"file_name\":file_out, \"site_id\":[joint_boxes_data['site_id'].iloc[i]], \n",
    "                                                        \"species\":joint_boxes_data['species_code'].iloc[i], \"occurrence\":[joint_boxes_data['occurrence'].iloc[i]]})])\n",
    "    if i % 100 == 0:\n",
    "        print(f\"    Processed {i}/{len(joint_boxes_data)} ({round(i/len(joint_boxes_data)*100, 2)} %) clips   \", end = \"\\r\")\n",
    "            \n",
    "bsg_metadata.reset_index(drop=True, inplace=True)\n",
    "bsg_metadata.to_csv(bsg_metadata_out+ 'BSG_soundscapes.csv', index=False)\n",
    "                                                      \n",
    "bsg_labels.reset_index(drop=True, inplace=True)\n",
    "bsg_labels.to_csv(bsg_metadata_out+ 'BSG_labels.csv', index=False)\n",
    "                                                      \n",
    "print(\"Clips complete.                                                               \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6070c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clips saved: 111248\n",
      "Files in metadata: 111248\n",
      "Difference between sets:\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# check metadata and files\n",
    "audio_clips = os.listdir(path_to_birdsound_library) \n",
    "print(f\"Clips saved: {len(audio_clips)}\")\n",
    "print(f\"Files in metadata: {len(bsg_metadata)}\")\n",
    "print(\"Difference between sets:\")\n",
    "print(list(set(bsg_metadata['file_name']) - set(audio_clips)))\n",
    "print(list(set(audio_clips) - set(bsg_metadata['file_name'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
